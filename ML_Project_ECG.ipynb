{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKCE4rKeR4Cm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbEIh8WoR6Xg"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/1705074/Academic/Level_4_Term_2/CSE_472_Machine_Learning_Sessional/Project/npy_files\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dxh6Y96UWRF"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roRzK3WVVCS_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utQIZY3oav0g"
      },
      "outputs": [],
      "source": [
        "def get_mini_batch(input_X, label, batch_size):\n",
        "    for i in range(0, len(input_X), batch_size):\n",
        "        yield input_X[i: i + batch_size], label[i: i + batch_size]\n",
        "\n",
        "def get_mini(input_X, batch_size):\n",
        "    for i in range(0, len(input_X), batch_size):\n",
        "        yield input_X[i: i + batch_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcmV-BF1UaX1"
      },
      "outputs": [],
      "source": [
        "def load_images_from_npy():\n",
        "    covid_images = np.load('./covid.npy', allow_pickle=True)\n",
        "    hb_images = np.load('./hb.npy', allow_pickle=True)\n",
        "    mi_images = np.load('./mi.npy', allow_pickle=True)\n",
        "    normal_images = np.load('./normal.npy', allow_pickle=True)\n",
        "    pmi_images = np.load('./pmi.npy', allow_pickle=True)\n",
        "\n",
        "\n",
        "    # covid - 0\n",
        "    # hb - 1\n",
        "    # mi - 2\n",
        "    # normal - 3\n",
        "    # pmi - 4\n",
        "\n",
        "    # fill labels 0 for covid, 1 for hb, 2 for mi, 3 for normal, 4 for pmi\n",
        "    labels = [0] * (len(covid_images) // 13)\n",
        "    labels.extend([1] * (len(hb_images) // 13))\n",
        "    labels.extend([2] * (len(mi_images) // 13))\n",
        "    labels.extend([3] * (len(normal_images) // 13))\n",
        "    labels.extend([4] * (len(pmi_images) // 13))\n",
        "\n",
        "    images = np.concatenate((covid_images, hb_images, mi_images, normal_images, pmi_images), axis=0)\n",
        "\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "\n",
        "def combine_leads_of_image(data):\n",
        "  \n",
        "    # store each 13 lead images in a list of 13*28*28 arrays\n",
        "    images = []\n",
        "\n",
        "    for X in get_mini(data, 13):\n",
        "        images.append(X)\n",
        "\n",
        "    images = np.array(images)\n",
        "    return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgEIP-4TU_fK",
        "outputId": "11a35f87-04f1-486b-8100-42142069572e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(35698, 28, 28) (2746,) (28, 28)\n",
            "(2746, 13, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "images, labels = load_images_from_npy()\n",
        "print(images.shape, labels.shape, images[0].shape)\n",
        "\n",
        "images = combine_leads_of_image(images)\n",
        "\n",
        "print(images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eitQtWHoGCl2"
      },
      "source": [
        "## **Train Validation Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5my2NzZbGBQJ"
      },
      "outputs": [],
      "source": [
        "def train_val_split(images, labels, split_factor=0.8, shuffle=True):\n",
        "    # if shuffle:\n",
        "    random_indices = np.random.choice(len(images), len(images), replace=False)\n",
        "    training_data_full = images[random_indices]\n",
        "    training_label_full = labels[random_indices]\n",
        "\n",
        "    split_index = int(split_factor * len(training_data_full))\n",
        "    split_index_test = int(0.9 * len(training_data_full))\n",
        "\n",
        "    training_data = training_data_full[:split_index]\n",
        "    training_label = training_label_full[:split_index]\n",
        "\n",
        "    validation_data = training_data_full[split_index : split_index_test]\n",
        "    validation_label = training_label_full[split_index : split_index_test]\n",
        "\n",
        "    test_data = training_data_full[split_index_test :]\n",
        "    test_label = training_label_full[split_index_test :]\n",
        "\n",
        "    return training_data, training_label, validation_data, validation_label, test_data, test_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJeCve_HTumm"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHh3om_aR779"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.optim.lr_scheduler as lr_scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UljrkU5ETx5t"
      },
      "source": [
        "## **Building Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqKJYMiQT6Xc"
      },
      "outputs": [],
      "source": [
        "class HybridNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HybridNetwork, self).__init__()\n",
        "\n",
        "        # build LeNet-5 using nn.Sequential\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.LazyConv3d(out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.LazyConv3d(out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
        "\n",
        "            # nn.LazyConv3d(out_channels=256, kernel_size=3, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            # nn.MaxPool3d(kernel_size=2, stride=2),\n",
        "\n",
        "            # nn.LazyConv3d(out_channels=512, kernel_size=3, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            # nn.MaxPool3d(kernel_size=2, stride=2),\n",
        "\n",
        "            # nn.LazyConv3d(out_channels=512, kernel_size=3, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            # nn.MaxPool3d(kernel_size=2, stride=2),\n",
        "\n",
        "            # nn.LSTM(input_size=(8, 512), batch_first=True, dropout=0.25, bidirectional=True, hidden_size=10)\n",
        "\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.LazyLinear(64),\n",
        "            nn.ReLU(),\n",
        "            nn.LazyLinear(5),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, input_X):\n",
        "\n",
        "      for layer in self.layers:\n",
        "        input_X = layer(input_X)\n",
        "        # print(input_X.shape)\n",
        "\n",
        "      return input_X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEri_DS7UB4t"
      },
      "source": [
        "## **Building Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9u4XJLeUHGt"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "  def __init__(self):\n",
        "    self.network = HybridNetwork()\n",
        "    self.num_epochs = 10\n",
        "    self.batch_size = 4\n",
        "    self.grad_clipping = 10.0\n",
        "    self.optimizer_type = 'adamax'\n",
        "    self.lr = 0.001\n",
        "    self.num_output_units = 5\n",
        "\n",
        "    self.init_optimizer()\n",
        "\n",
        "  def init_optimizer(self):\n",
        "    parameters = [p for p in self.network.parameters() if p.requires_grad]\n",
        "    if self.optimizer_type == 'sgd':\n",
        "        self.optimizer = optim.SGD(parameters, self.lr,\n",
        "                                    momentum=0.4,\n",
        "                                    weight_decay=0)\n",
        "    elif self.optimizer_type == 'adamax':\n",
        "        self.optimizer = optim.Adamax(parameters,\n",
        "                                    lr=self.lr,\n",
        "                                    weight_decay=0)\n",
        "    else:\n",
        "        raise RuntimeError('Unsupported optimizer: %s' %\n",
        "                            self.optimizer_type)\n",
        "    self.scheduler = lr_scheduler.MultiStepLR(self.optimizer, milestones=[10, 15], gamma=0.5)\n",
        "\n",
        "  \n",
        "  def normalize(self, X):\n",
        "    # apply standard normalization on x. mean = 0, std = 1\n",
        "    X = (X - np.mean(X)) / np.std(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "  def reshape_image(self, input_X, label):\n",
        "\n",
        "      # X_full = input_X.reshape(input_X.shape[0], 1, input_X.shape[1], input_X.shape[2])\n",
        "\n",
        "      X_full = input_X.reshape(input_X.shape[0], 1, input_X.shape[1], 28, 28)\n",
        "      y_full = np.eye(self.num_output_units)[label.astype(int)]\n",
        "\n",
        "      return X_full, y_full\n",
        "\n",
        "  def report_num_trainable_parameters(self):\n",
        "    num_parameters = 0\n",
        "    for p in self.network.parameters():\n",
        "        if p.requires_grad:\n",
        "            sz = list(p.size())\n",
        "\n",
        "            num_parameters += np.prod(sz)\n",
        "    print('Number of parameters: ', num_parameters)\n",
        "\n",
        "\n",
        "  def evaluate(self, validation_data, validation_label):\n",
        "    self.network.eval()\n",
        "\n",
        "    X = validation_data\n",
        "    y = validation_label\n",
        "\n",
        "    X = self.normalize(X)\n",
        "    cnt = 0\n",
        "\n",
        "    for X_batch, y_batch in tqdm(get_mini_batch(input_X=X, label=y, batch_size=self.batch_size), desc=\"Evaluating\"):\n",
        "        \n",
        "        # reshape to appropriate format\n",
        "        X, y_true = self.reshape_image(X_batch, y_batch)\n",
        "\n",
        "        # convert to float32 and convert to torch tensor\n",
        "        X = X.astype(np.float32)\n",
        "        y_true = y_true.astype(np.float32)\n",
        "        X = torch.from_numpy(X)\n",
        "        y_true = torch.from_numpy(y_true)\n",
        "\n",
        "        pred_proba = self.network(X)\n",
        "\n",
        "        y_pred = torch.argmax(pred_proba, dim=1)\n",
        "        y_true = torch.argmax(y_true, dim=1)\n",
        "\n",
        "        # print(y_pred)\n",
        "        # print(y_true)   \n",
        "\n",
        "        cnt += torch.sum(y_pred == y_true).item()\n",
        "    \n",
        "    print(f'accuracy: {cnt/validation_data.shape[0]}')\n",
        "\n",
        "\n",
        "  def train(self, input_X, label):\n",
        "    self.updates = 0\n",
        "    iter_cnt, num_iter = 0, (len(input_X) + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "    for X_batch, y_batch in tqdm(get_mini_batch(input_X=input_X, label=label, batch_size=self.batch_size), desc=\"Training\"):\n",
        "        \n",
        "        # reshape to appropriate format\n",
        "        X, y_true = self.reshape_image(X_batch, y_batch)\n",
        "        \n",
        "        # convert to float32 and convert to torch tensor\n",
        "        X = X.astype(np.float32)\n",
        "        y_true = y_true.astype(np.float32)\n",
        "        X = torch.from_numpy(X)\n",
        "        y_true = torch.from_numpy(y_true)\n",
        "\n",
        "        pred_proba = self.network(X)\n",
        "\n",
        "        loss = F.cross_entropy(pred_proba, y_true)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm(self.network.parameters(), self.grad_clipping)\n",
        "\n",
        "        # Update parameters\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.updates += 1\n",
        "        iter_cnt += 1\n",
        "\n",
        "        if self.updates % 1000 == 0:\n",
        "            print('Iter: %d/%d, Loss: %f' % (iter_cnt, num_iter, loss.item()))\n",
        "    \n",
        "    self.scheduler.step()\n",
        "    print('LR:', self.scheduler.get_last_lr()[0])            \n",
        "\n",
        "\n",
        "  def fit(self, training_data, training_label, validation_data, validation_label):\n",
        "\n",
        "      X = training_data\n",
        "      y = training_label\n",
        "\n",
        "      X = self.normalize(X)\n",
        "\n",
        "      self.evaluate(validation_data, validation_label)\n",
        "\n",
        "      self.network.train()\n",
        "\n",
        "      for epoch in range(self.num_epochs):\n",
        "          self.train(input_X=X, label=y)\n",
        "          print(\"Epoch: \", epoch)\n",
        "\n",
        "          self.evaluate(validation_data, validation_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p32Im66FUMXN"
      },
      "source": [
        "## **Train, Validation Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfXJClppSIqV"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    training_data, training_label, validation_data, validation_label, test_data, test_label = train_val_split(images=images, labels=labels, split_factor=0.8, shuffle=True)\n",
        "    print(training_data.shape, training_label.shape, validation_data.shape, validation_label.shape, test_data.shape, test_label.shape)\n",
        "   \n",
        "    model = Model()\n",
        "    model.fit(training_data, training_label, validation_data, validation_label)\n",
        "\n",
        "    model.evaluate(test_data, test_label)\n",
        "\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "6656630dda156a19c310572b28ca8f6f6f691585771c146f49724423d30b9beb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
